<h3>Starting kit</h3>
<p>Everything you need to create a <strong>sample code submission</strong> and test it can be downloaded from the Files tab, under <strong>"Starting Kit"</strong>. We also provide small sample datasets under&nbsp;<strong>"Public Data"</strong>.</p>
<h3>Larger practice datasets</h3>
<table border="1" cellspacing="1" cellpadding="1" align="left">
<tbody>
<tr>
<td style="text-align: left;">&nbsp;#&nbsp;</td>
<td style="text-align: left;">&nbsp;Name</td>
<td style="text-align: left;">&nbsp;Domain</td>
<td style="text-align: left;">&nbsp;Size</td>
<td style="text-align: left;">&nbsp;Source</td>
<td style="text-align: left;">
<p>&nbsp;Data (w/o test labels)</p>
</td>
<td style="text-align: left;">&nbsp;Test labels</td>
</tr>
<tr>
<td>&nbsp;1</td>
<td>&nbsp;ADULT</td>
<td>&nbsp;tabular</td>
<td>&nbsp;1.9 MB</td>
<td>&nbsp;<a href="https://archive.ics.uci.edu/ml/datasets/adult" target="_blank">Adult</a></td>
<td>&nbsp;<a href="../../my/datasets/download/c92700af-3b0f-432a-b784-898383fda1b2">adult.data</a></td>
<td>&nbsp;<a href="../../my/datasets/download/4f48064f-3da6-48a5-9ed6-9a2b260c47c4">adult.solution</a></td>
</tr>
<tr>
<td>&nbsp;2</td>
<td>&nbsp;KATZE</td>
<td>&nbsp;video</td>
<td>&nbsp;1.94 GB</td>
<td style="text-align: center;">&nbsp;<a href="http://www.nada.kth.se/cvap/actions/" target="_blank">KTH Human Actions</a></td>
<td>&nbsp;<a href="../../my/datasets/download/49288e21-67eb-4984-b8fe-dc23e53e37b3">katze.data</a>&nbsp;</td>
<td>&nbsp;<a href="../../my/datasets/download/0cf44915-ac2f-4e5f-9b12-432c1ddac1d3">katze.solution</a>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;3</td>
<td>&nbsp;TYPHOON</td>
<td>&nbsp;speech</td>
<td>&nbsp;599 MB</td>
<td style="text-align: left;">&nbsp;<a href="https://www.kaggle.com/nltkdata/timitcorpus" target="_blank">TIMIT phonetic</a></td>
<td>&nbsp;<a href="../../my/datasets/download/ad367a65-fc58-4647-b53c-b6dd0a26f429">typhoon.data</a></td>
<td>&nbsp;<a href="../../my/datasets/download/b78d686d-9f47-4cbc-8f25-ede55e0b1eee">typhoon.solution</a></td>
</tr>
<tr>
<td>&nbsp;4</td>
<td>&nbsp;TWEET</td>
<td>&nbsp;text</td>
<td>&nbsp;604 MB</td>
<td style="text-align: left;">&nbsp;<a href="http://qwone.com/~jason/20Newsgroups/" target="_blank">20 Newsgroups</a></td>
<td>&nbsp;<a href="../../my/datasets/download/fea6c7bb-b398-4757-924c-b20f16bf4fde">tweet.data</a>&nbsp;</td>
<td>&nbsp;<a href="../../my/datasets/download/724b6bef-f223-4828-881c-2b44b286dd24">tweet.solution</a></td>
</tr>
<tr>
<td style="text-align: left;">&nbsp;5</td>
<td style="text-align: left;">&nbsp;MUNSTER</td>
<td style="text-align: left;">&nbsp;image</td>
<td style="text-align: left;">&nbsp;17.2 MB</td>
<td style="text-align: left;">&nbsp;<a href="http://yann.lecun.com/exdb/mnist/" target="_blank">MNIST</a></td>
<td style="text-align: left;">&nbsp;<a href="../../my/datasets/download/9ecba9a7-a066-4888-9c00-e0235617446d">munster.data</a>&nbsp;</td>
<td style="text-align: left;">&nbsp;<a href="../../my/datasets/download/feb8a3b3-22ed-4dfc-8935-664ca7e15f2f">munster.solution</a></td>
</tr>
</tbody>
</table>
<h3>&nbsp;</h3>
<h3>&nbsp;</h3>
<h3>&nbsp;</h3>
<div>
<p>&nbsp;</p>
<p><strong>Usage</strong>: Dowload and put only ONE pair of data and solution (e.g. adult.data and adult.solution) in the directory AutoDL_sample_data/ (after unzipping). You can also put them in a directory of your choice and specify the path when calling `run_local_test.py`.</p>
<h3>Some statistics/info on these datasets&nbsp;</h3>
<table border="1" cellspacing="1" cellpadding="1" align="left">
<tbody>
<tr>
<td style="text-align: left;">&nbsp;#&nbsp;</td>
<td style="text-align: left;">&nbsp;Name</td>
<td style="text-align: left;">
<p>&nbsp;num_train</p>
</td>
<td style="text-align: left;">
<p>&nbsp;num_test</p>
</td>
<td style="text-align: left;">
<p>&nbsp;col_count</p>
</td>
<td style="text-align: left;">
<p>&nbsp;row_count</p>
</td>
<td style="text-align: left;">
<p>&nbsp;sequence_size</p>
</td>
<td style="text-align: left;">
<p>&nbsp;output_dim</p>
</td>
<td style="text-align: left;">
<p>&nbsp;has_locality_col</p>
</td>
<td>
<p style="text-align: left;">&nbsp;has_locality_row</p>
</td>
</tr>
<tr>
<td>&nbsp;1</td>
<td>&nbsp;ADULT</td>
<td>&nbsp;39074</td>
<td>&nbsp;9768</td>
<td>&nbsp;24</td>
<td>&nbsp;1</td>
<td>&nbsp;1</td>
<td>&nbsp;3</td>
<td>&nbsp;false</td>
<td>&nbsp;false</td>
</tr>
<tr>
<td>&nbsp;2</td>
<td>&nbsp;KATZE</td>
<td>&nbsp;1528</td>
<td>&nbsp;863</td>
<td>&nbsp;160</td>
<td>&nbsp;120</td>
<td>&nbsp;181</td>
<td>&nbsp;6</td>
<td>&nbsp;true</td>
<td>&nbsp;true</td>
</tr>
<tr>
<td>&nbsp;3</td>
<td>&nbsp;TYPHOON</td>
<td>&nbsp;177080</td>
<td>&nbsp;64145</td>
<td>&nbsp;1</td>
<td>&nbsp;1</td>
<td>&nbsp;1934</td>
<td>&nbsp;71</td>
<td>&nbsp;false</td>
<td>&nbsp;false</td>
</tr>
<tr>
<td>&nbsp;4</td>
<td>&nbsp;TWEET</td>
<td>&nbsp;11314</td>
<td>&nbsp;7532</td>
<td>&nbsp;50</td>
<td>&nbsp;1</td>
<td>&nbsp;473</td>
<td>&nbsp;20</td>
<td>&nbsp;false</td>
<td>&nbsp;false</td>
</tr>
<tr>
<td>&nbsp;5</td>
<td>&nbsp;MUNSTER</td>
<td>&nbsp;60000</td>
<td>&nbsp;10000</td>
<td>&nbsp;28</td>
<td>&nbsp;28</td>
<td>&nbsp;1</td>
<td>&nbsp;10</td>
<td>&nbsp;true</td>
<td>&nbsp;true</td>
</tr>
</tbody>
</table>
<h3>&nbsp;</h3>
<h3>&nbsp;</h3>
<h3>&nbsp;</h3>
<div>
<p>&nbsp;</p>
</div>
<p>The columns correspond to:</p>
<p>num_train/num_test: number of training/test examples</p>
<p>col_count/row_count/sequence_size: shape parameters of the examples (in AutoDL challenge, every example is represented by a 3D tensor with axes x, y and t (t for time))</p>
<p>output_dim: number of classes</p>
<p>has_locality_col/has_locality_row:&nbsp;indicate if column/row indices carry some structure/information based on closeness of indices, like in an image or in a video.</p>
</div>
<h3>Data format</h3>
<p>Data are not pre-processed in a uniform feature vector representation; they include all data types representable as spatio-temporal sequences. We use a generic data format based on&nbsp;<a href="https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data">TFRecords</a>, used by <a href="https://www.tensorflow.org/">TensorFlow</a>. This format allows us to format any 2D+time data, including text, speech, image, video, etc. This will not however impose to participants to use deep learning algorithms nor even Tensorflow. If you want to practice designing algorithms with your own datasets, we provide <a href="https://github.com/zhengying-liu/autodl-contrib/tree/master/tfrecord_format">instructions to help you</a>.</p>
