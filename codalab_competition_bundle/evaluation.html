<h2>Quick start</h2>
<p>This is a challenge with code submission. We provide 3 baseline methods for test purposes:</p>
<p><strong><a href="../../my/datasets/download/94d10596-86f3-41fa-87ed-b83bbb3104c0">Baseline 0</a>: Make constant (zero) predictions</strong></p>
<p><strong><a href="../../my/datasets/download/0c066b28-71ca-4ea0-b079-7f999071cf9f">Baseline 1</a>: Linear classifier</strong></p>
<p><strong><a href="../../my/datasets/download/de8523bb-3818-46e4-b8da-e4385dbbbe1b">Baseline 2</a>: a 3D Convolutional Neural Network</strong></p>
<p>To make a test submission, download one of the baseline methods, click on the blue button "<strong>Upload a Submission</strong>" in the upper right corner of the page and re-upload it (you must click first the orange tab "All datasets" for this to work; known bug).&nbsp;To check progress on your submissions go to the "<strong>My Submissions</strong>" tab. Your best submission is shown on the leaderboard visible under the "<strong>Results</strong>" tab.</p>
<h2>Complete starting kit</h2>
<p>The starting kit contains everything you need to create <strong>your own code submission</strong>&nbsp;(just by modifying the file model.py) and to test it on your local computer, in conditions identical to those of the Codalab platform. <strong><br /></strong></p>
<center>
<h3><span><a href="https://github.com/zhengying-liu/autodl_starting_kit_stable" target="_blank">Download the Starting Kit</a></span></h3>
<p style="text-align: left;">This includes a <strong>jupyter notebook <a href="https://nbviewer.jupyter.org/github/zhengying-liu/autodl_starting_kit_stable/blob/master/tutorial.ipynb" target="_blank">tutorial.ipynb</a></strong> with step-by-step instructions. The interface is simple and generic: you must supply a Python class <strong>model.py</strong> with:</p>
<ul style="text-align: left;">
<li>a constructor</li>
<li>a `done_training` attribute</li>
<li>a `train` method</li>
<li>a `test` method</li>
</ul>
<p style="text-align: left;">To make submissions,&nbsp;<strong>zip model.py</strong>, then use the "<strong>Upload a Submission</strong>" button. That's it!</p>
</center>
<h2>Larger practice datasets</h2>
<p style="text-align: left;">The starting kit contains sample data, but you may want to develop your code with larger practice datasets. Download and<strong> put only ONE pair of data and solution</strong> (e.g. munster.data and munster.solution) in the directory AutoDL_public_data/&lt;dataset_name&gt; (after unzipping). You can also put them in a directory of your choice and specify the path when calling run_local_test.py.&nbsp;</p>
<table border="1" cellspacing="1" cellpadding="1" align="left">
<tbody>
<tr>
<td>&nbsp;#&nbsp;</td>
<td>&nbsp;Name</td>
<td>&nbsp;Domain</td>
<td>&nbsp;Size</td>
<td>&nbsp;Source</td>
<td>
<p>&nbsp;Data (w/o test labels)</p>
</td>
<td>&nbsp;Test labels</td>
</tr>
<tr>
<td>&nbsp;1</td>
<td>&nbsp;Munster</td>
<td>&nbsp;HWR</td>
<td>&nbsp;18 MB</td>
<td>&nbsp;<a href="http://yann.lecun.com/exdb/mnist/" target="_blank">MNIST</a></td>
<td>&nbsp;<a href="../../my/datasets/download/6662aa6e-75ab-439c-bf98-97dd11401053">munster.data</a></td>
<td>&nbsp;<a href="../../my/datasets/download/f3a61a40-b1f1-4ded-bc55-fb730a12f4c4">munster.solution</a></td>
</tr>
<tr>
<td>&nbsp;2</td>
<td>&nbsp;Chucky</td>
<td>&nbsp;Objects</td>
<td>&nbsp;128 MB</td>
<td>&nbsp;<a href="https://www.cs.toronto.edu/~kriz/cifar.html">Cifar-100</a></td>
<td>&nbsp;<a href="../../my/datasets/download/d06aa5fc-1fb5-4283-8e05-abed4ccdd975">chucky.data</a></td>
<td>&nbsp;<a href="../../my/datasets/download/29932707-21cc-4670-a7db-cdc246a8ab71">chucky.solution</a></td>
</tr>
<tr>
<td>&nbsp;3</td>
<td>&nbsp;Pedro</td>
<td>&nbsp;People</td>
<td>&nbsp;377 MB</td>
<td>&nbsp;<a href="https://github.com/xh-liu/HydraPlus-Net"><span style="font-size: 10pt; font-family: Arial; font-style: normal;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;PA-100K&quot;}" data-sheets-userformat="{&quot;2&quot;:513,&quot;3&quot;:{&quot;1&quot;:0},&quot;12&quot;:0}">PA-100K</span></a></td>
<td>&nbsp;<a href="../../my/datasets/download/61a074cd-e909-4d49-b313-7da0d4f7dc8b">pedro.data</a></td>
<td>&nbsp;<a href="../../my/datasets/download/852c1e68-5e91-477e-bef0-824b503814e8">pedro.solution</a></td>
</tr>
<tr>
<td>&nbsp;4</td>
<td>&nbsp;Decal</td>
<td>&nbsp;Aerial</td>
<td>&nbsp;73 MB</td>
<td>&nbsp;<a href="http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html">NWPU VHR-10</a></td>
<td>&nbsp;<a href="../../my/datasets/download/dfd93c39-e0d4-41b2-b332-4dd002676e05">decal.data</a></td>
<td>&nbsp;<a href="../../my/datasets/download/d72cba79-3051-4779-b624-e50335aad874">decal.solution</a></td>
</tr>
<tr>
<td>&nbsp;5</td>
<td>&nbsp;Hammer</td>
<td>&nbsp;Medical</td>
<td>&nbsp;111 MB</td>
<td>&nbsp;<a href="https://www.nature.com/articles/sdata2018161">Ham10000</a></td>
<td>&nbsp;<a href="../../my/datasets/download/eb569948-72f0-4002-8e4d-479a27766cbf">hammer.data</a></td>
<td>&nbsp;<a href="../../my/datasets/download/c3729c98-4755-47a2-b764-a4159c5ca152">hammer.solution</a></td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h2>&nbsp;</h2>
<h2>&nbsp;</h2>
<p>&nbsp;</p>
<table border="1" cellspacing="1" cellpadding="1" align="left">
<tbody>
<tr>
<td>
<p>&nbsp;#&nbsp;</p>
</td>
<td>&nbsp;Name</td>
<td>
<p>&nbsp;num_train</p>
</td>
<td>
<p>&nbsp;num_test</p>
</td>
<td>
<p>&nbsp;sequence_size</p>
</td>
<td>
<p>&nbsp;row_count</p>
</td>
<td>
<p>&nbsp;col_count</p>
</td>
<td>
<p>&nbsp;num_channels</p>
</td>
<td>&nbsp;output_dim&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;1</td>
<td>&nbsp;Munster</td>
<td>&nbsp;60000</td>
<td>&nbsp;10000</td>
<td>&nbsp;1</td>
<td>&nbsp;28</td>
<td>&nbsp;28</td>
<td>&nbsp;1</td>
<td>&nbsp;10</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;2</td>
<td>&nbsp;Chucky</td>
<td>&nbsp;48061</td>
<td>&nbsp;11939</td>
<td>&nbsp;1</td>
<td>&nbsp;32</td>
<td>&nbsp;32</td>
<td>&nbsp;3</td>
<td>&nbsp;100</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;3</td>
<td>&nbsp;Pedro</td>
<td>&nbsp;80095</td>
<td>&nbsp;19905</td>
<td>&nbsp;1</td>
<td>&nbsp;-1</td>
<td>&nbsp;-1</td>
<td>&nbsp;3</td>
<td>&nbsp;26</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;4</td>
<td>&nbsp;Decal</td>
<td>&nbsp;634</td>
<td>&nbsp;166</td>
<td>&nbsp;1</td>
<td>&nbsp;-1</td>
<td>&nbsp;-1</td>
<td>&nbsp;3</td>
<td>&nbsp;11</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;5</td>
<td>&nbsp;Hammer</td>
<td>&nbsp;8050</td>
<td>&nbsp;1965</td>
<td>&nbsp;1</td>
<td>&nbsp;400</td>
<td>&nbsp;300</td>
<td>&nbsp;3</td>
<td>&nbsp;7</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<ul>
<li>num_train/num_test: number of training/test examples</li>
</ul>
<ul>
<li>sequence_size/row_count/col_count/num_channels: shape parameters of the examples (in AutoCV challenge, every example is represented by a 4D tensor with axes t, x, y and c (t for time and c for channel)). A <em>row_count</em> or <em>col_count</em> of -1 means the value varies from one example to another.</li>
<li>output_dim: number of classes</li>
<li>has_locality_col (resp. has_locality_row): A flag indicating whether rows (resp. columns) in the tensor correspond to variables&nbsp;<span>interrelated</span> with some underlying topological structure ,reflected by the closeness of indices, like in images or videos.</li>
</ul>
<p>These data were re-formatted from original public datasets. If you use them, please make sure to acknowledge the original data donnors (see "Source" in the data table) and check the tems of use.</p>
<p><strong>To download all public datasets at once:</strong></p>
<pre>cd autodl_starting_kit_stable<br />python download_public_datasets.py</pre>
<h2>Format and use your own datasets</h2>
<p>Raw data are preserved, but formatted in a generic data format based on&nbsp;<a href="https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data">TFRecords</a>, used by&nbsp;<a href="https://www.tensorflow.org/">TensorFlow</a>. However, this will not impose to participants to use deep learning algorithms nor even Tensorflow. If you want to practice designing algorithms with your own datasets, <a href="https://github.com/zhengying-liu/autodl-contrib/blob/master/README.md">follow these steps</a>.&nbsp;</p>
<h2>Competition protocol</h2>
<p>This challenge has <strong>1 single phase</strong>. Your code is blind tested on five datasets. And the score you obtain (with your last submission) will be used for final evaluation. There is no hidden leaderboard.</p>
<p>Code submitted is trained and tested automatically, without any human intervention. Code submitted is run on all five datasets in parallel on separate compute workers, each one with its own time budget.&nbsp;</p>
<p>The identities of the datasets used for blind testing on the platform are concealed.&nbsp;The data are provided in a&nbsp;<strong>raw form</strong>&nbsp;(no feature extraction) to encourage researchers to use Deep Learning methods performing automatic feature learning, although this is NOT a requirement. All problems are&nbsp;<strong>multi-label classification&nbsp;</strong>problems. The tasks are constrained by a&nbsp;<strong>time budget</strong>.&nbsp;</p>
<p>Here is some pseudo-code of the evaluation protocol:</p>
<pre><code># For each dataset, our evaluation program calls the model constructor:
M =&nbsp;<strong>Model</strong>(metadata=dataset_metadata)
# Initialize
remaining_time budget = overall_time_budget
start_time = time()
# Ingestion program calls multiple times train and test:
repeat until M.done_training or remaining_time_budget &lt; 0
{
<strong>  M.train</strong> (training_data, remaining_time_budget)
  remaining_time_budget = start_time + overall_time_budget - time.time()
  results = <strong>M.test</strong>(test_data, remaining_time_budget)
  remaining_time_budget = start_time + overall_time_budget - time.time()<br />  # Results made available to scoring program (run in separate container)
  save(results)
}
</code></pre>
<p><span>It is the responsibility of the participants to make sure</span>&nbsp;that neither the "train" nor the "test" methods exceed the &ldquo;remaining_time_budget&rdquo;. The method &ldquo;train&rdquo; can choose to manage its time budget such that it trains in varying time increments.&nbsp;There is pressure that it does not use all "overall_time_budget" at the first iteration because we use the area under the learning curve as metric, and the <strong>time axis is in log-scale</strong>.</p>
<h2>Metrics</h2>
<p><span>The participants can train in batches of pre-defined duration to incrementally improve their performance</span>, until the time limit is attained. In this way we can plot learning curves:&nbsp;<strong>"performance" as a function of time</strong>. Each time the "train" method terminates, the "test" method is called and the results are saved, so the scoring program can use them, together with their time stamp.</p>
<p>We treat both multi-class and multi-label problems alike. Each label/class is considered a separate binary classification problem. We measure performance with the average over all labels of</p>
<p><strong>balanced_accuracy = (1/2) (TPR + TNR).</strong></p>
<p>For each dataset, we compute the&nbsp;<strong>area under the learning curve</strong>&nbsp;(by the trapeze method), i.e. the area of mean_balanced_accuracy as a function of log(1+time), where "time" is the cumulative time in seconds of training and testing. The&nbsp;<strong>overall ranking</strong>&nbsp;is made by averaging the ranks obtained on the 5 datasets.&nbsp;</p>
<p>Examples of learning curves:&nbsp;<img class="img-responsive" src="{{ ASSET_BASE_URL }}/learning-curve-ex1-tweet.png" alt="" />&nbsp;<img class="img-responsive" src="{{ ASSET_BASE_URL }}/learning-curve-ex2-tsunami.png" alt="" />&nbsp;</p>
<p>&nbsp;</p>