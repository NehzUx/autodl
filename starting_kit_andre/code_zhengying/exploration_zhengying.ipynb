{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evariste/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "root_dir = '../'\n",
    "from sys import path; path.append(root_dir)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AutoDLDataset\n",
    "from dataset import AutoDLMetadata\n",
    "\n",
    "path_to_data = \"/Users/evariste/projects/autodl/codalab_competition_bundle/AutoDL_input_data_1/mnist1/train\"\n",
    "\n",
    "dataset_name = \"mnist\"\n",
    "data_set = AutoDLDataset(path_to_data)\n",
    "data_set.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata = data_set.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_metadata_info(metadata):\n",
    "    bundle_size = metadata.get_bundle_size()\n",
    "    print(\"Number of bundles: \", bundle_size)\n",
    "    print(\"Output size: \", metadata.get_output_size())\n",
    "    print(\"Sequence size: \", metadata.get_sequence_size())\n",
    "    for bundle_index in range(bundle_size):\n",
    "        print(\"Info on Bundle #\", bundle_index, \": \")\n",
    "        if metadata.is_compressed(bundle_index):\n",
    "            print(\"  Bundle format: COMPRESSED\")\n",
    "        elif metadata.is_sparse(bundle_index):\n",
    "            print(\"  Bundle format: SPARSE\")\n",
    "        else:\n",
    "            print(\"  Bundle format: DENSE\")\n",
    "        print(\"  Matrix size: \", metadata.get_matrix_size(bundle_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bundles:  1\n",
      "Output size:  10\n",
      "Sequence size:  1\n",
      "Info on Bundle # 0 : \n",
      "  Bundle format: DENSE\n",
      "  Matrix size:  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print_metadata_info(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = data_set.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_example, next_label = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((?, 1, ?, ?), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.parsing_ops.VarLenFeature"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf.VarLenFeature(tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((?, 1, ?, ?), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "col_count = 28\n",
    "row_count = 28\n",
    "sequence_size = 1\n",
    "output_dim = 10\n",
    "\n",
    "dataset = data_set.get_dataset()\n",
    "# dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "next_example, next_label = iterator.get_next()\n",
    "# next_example_flat = tf.layers.flatten(next_example)\n",
    "next_example_flat = tf.reshape(next_example, [-1, col_count*row_count*sequence_size])\n",
    "example_flatten = tf.layers.flatten(next_example)\n",
    "\n",
    "logits = tf.layers.dense(inputs=next_example_flat, units=output_dim)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=next_label, logits=logits)\n",
    "\n",
    "training_op = tf.train.AdagradOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/g_/rs9p7mjx0c10qk25vqkmmw380000gn/T/tmp4dswpfb8\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/g_/rs9p7mjx0c10qk25vqkmmw380000gn/T/tmp4dswpfb8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c2255fc50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Number of training files: 7.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "################################################## model_fn train (?, 784)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/g_/rs9p7mjx0c10qk25vqkmmw380000gn/T/tmp4dswpfb8/model.ckpt.\n",
      "INFO:tensorflow:loss = 156.51836, step = 1\n",
      "INFO:tensorflow:global_step/sec: 295.446\n",
      "INFO:tensorflow:loss = 24.650639, step = 101 (0.340 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/g_/rs9p7mjx0c10qk25vqkmmw380000gn/T/tmp4dswpfb8/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 17.524172.\n",
      "************************************************** Begin evaluation!\n",
      "INFO:tensorflow:Number of training files: 1.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "################################################## model_fn infer (?, 784)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/g_/rs9p7mjx0c10qk25vqkmmw380000gn/T/tmp4dswpfb8/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(10000, 10)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evariste/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "col_count = 28\n",
    "row_count = 28\n",
    "sequence_size = 1\n",
    "output_dim = 10\n",
    "batch_size = 30\n",
    "\n",
    "from dataset import AutoDLDataset\n",
    "from dataset import AutoDLMetadata\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "  input_layer = tf.reshape(features['x'], \n",
    "                           [-1, sequence_size*row_count*col_count])\n",
    "  print(\"#\"*50, \"model_fn\", mode, input_layer.shape)\n",
    "  logits = tf.layers.dense(inputs=input_layer, units=output_dim)\n",
    "  \n",
    "  predictions = {\n",
    "    # Generate predictions (for PREDICT and EVAL mode)\n",
    "    \"classes\": tf.argmax(input=logits, axis=1),\n",
    "    # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "    # `logging_hook`.\n",
    "    \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  assert mode == tf.estimator.ModeKeys.EVAL\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \n",
    "  # Create the Estimator\n",
    "  classifier = tf.estimator.Estimator(model_fn=model_fn)\n",
    "\n",
    "  # Set up logging for predictions\n",
    "  # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "  logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=50)\n",
    "  \n",
    "  path_to_train_data = \"/Users/evariste/projects/autodl/\" +\\\n",
    "    \"codalab_competition_bundle/AutoDL_input_data_1/mnist1/train\"\n",
    "\n",
    "  train_data_set = AutoDLDataset(path_to_train_data)\n",
    "  train_data_set.init()\n",
    "  dataset = train_data_set.get_dataset()\n",
    "  train_dataset_dict = dataset.map(lambda x, y: ({'x': x}, y))\n",
    "  \n",
    "  # Train the model\n",
    "  def train_input_fn():\n",
    "    iterator = train_dataset_dict.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "    \n",
    "  classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=200)#,\n",
    "      # hooks=[logging_hook])\n",
    "  \n",
    "  print(\"*\"*50, \"Begin evaluation!\")\n",
    "  # Evaluate the model and print results\n",
    "#   eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#       x={\"x\": eval_data},\n",
    "#       y=eval_labels,\n",
    "#       num_epochs=1,\n",
    "#       shuffle=False)\n",
    "#   eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "#   print(eval_results)\n",
    "  path_to_test_data = \"/Users/evariste/projects/autodl/\" +\\\n",
    "    \"codalab_competition_bundle/AutoDL_input_data_1/mnist1/test\"\n",
    "  test_data_set = AutoDLDataset(path_to_test_data)\n",
    "  test_data_set.init(batch_size=1000, repeat=False)\n",
    "  dataset = test_data_set.get_dataset()\n",
    "  test_dataset_dict = dataset.map(lambda x, y: ({'x': x}, y))\n",
    "  \n",
    "  def test_input_fn():\n",
    "    iterator = test_dataset_dict.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "  \n",
    "  res = []\n",
    "  test_results = classifier.predict(input_fn=test_input_fn)\n",
    "  res = [x['probabilities'] for x in test_results]\n",
    "  res = np.array(res)\n",
    "  print(res.shape)\n",
    "  \n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_fn = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.RepeatDataset"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a8fdbd20c2ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-9305617ae1f9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Fetch the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-efa7eb9789eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute for 100 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Compute for 100 epochs.\n",
    "for _ in range(100):\n",
    "  sess.run(iterator.initializer)\n",
    "  while True:\n",
    "    try:\n",
    "      sess.run(next_element)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-80-ae6a6bd838a2>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:219: retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x1c22c10e10>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x18215ce630>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x1c22c10a90>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.RepeatDataset"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haha = tf.data.Dataset.from_tensor_slices(np.random.uniform(size=(4,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haha = haha.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.RepeatDataset"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_set.get_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((?, 1, ?, ?), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dataset in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "class Dataset(builtins.object)\n",
      " |  Represents a potentially large set of elements.\n",
      " |  \n",
      " |  A `Dataset` can be used to represent an input pipeline as a\n",
      " |  collection of elements (nested structures of tensors) and a \"logical\n",
      " |  plan\" of transformations that act on those elements.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply(self, transformation_func)\n",
      " |      Apply a transformation function to this dataset.\n",
      " |      \n",
      " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
      " |      represented as functions that take one `Dataset` argument and return a\n",
      " |      transformed `Dataset`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```\n",
      " |      dataset = (dataset.map(lambda x: x ** 2)\n",
      " |                 .apply(group_by_window(key_func, reduce_func, window_size))\n",
      " |                 .map(lambda x: x ** 3))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        transformation_func: A function that takes one `Dataset` argument and\n",
      " |            returns a `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` returned by applying `transformation_func` to this\n",
      " |            dataset.\n",
      " |  \n",
      " |  batch(self, batch_size)\n",
      " |      Combines consecutive elements of this dataset into batches.\n",
      " |      \n",
      " |      NOTE: If the number of elements (`N`) in this dataset is not an exact\n",
      " |      multiple of `batch_size`, the final batch contain smaller tensors with\n",
      " |      shape `N % batch_size` in the batch dimension. If your program depends on\n",
      " |      the batches having the same shape, consider using the\n",
      " |      @{tf.contrib.data.batch_and_drop_remainder} transformation instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cache(self, filename='')\n",
      " |      Caches the elements in this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
      " |          directory on the filesystem to use for caching tensors in this Dataset.\n",
      " |          If a filename is not provided, the dataset will be cached in memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  concatenate(self, dataset)\n",
      " |      Creates a `Dataset` by concatenating given dataset with this dataset.\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3 }\n",
      " |      b = { 4, 5, 6, 7 }\n",
      " |      \n",
      " |      # Input dataset and dataset to be concatenated should have same\n",
      " |      # nested structures and output types.\n",
      " |      # c = { (8, 9), (10, 11), (12, 13) }\n",
      " |      # d = { 14.0, 15.0, 16.0 }\n",
      " |      # a.concatenate(c) and a.concatenate(d) would result in error.\n",
      " |      \n",
      " |      a.concatenate(b) == { 1, 2, 3, 4, 5, 6, 7 }\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        dataset: `Dataset` to be concatenated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  filter(self, predicate)\n",
      " |      Filters this dataset according to `predicate`.\n",
      " |      \n",
      " |      Args:\n",
      " |        predicate: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          scalar `tf.bool` tensor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  flat_map(self, map_func)\n",
      " |      Maps `map_func` across this dataset and flattens the result.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  interleave(self, map_func, cycle_length, block_length=1)\n",
      " |      Maps `map_func` across this dataset, and interleaves the results.\n",
      " |      \n",
      " |      For example, you can use `Dataset.interleave()` to process many input files\n",
      " |      concurrently:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Preprocess 4 files concurrently, and interleave blocks of 16 records from\n",
      " |      # each file.\n",
      " |      filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\", ...]\n",
      " |      dataset = (Dataset.from_tensor_slices(filenames)\n",
      " |                 .interleave(lambda x:\n",
      " |                     TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
      " |                     cycle_length=4, block_length=16))\n",
      " |      ```\n",
      " |      \n",
      " |      The `cycle_length` and `block_length` arguments control the order in which\n",
      " |      elements are produced. `cycle_length` controls the number of input elements\n",
      " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
      " |      transformation will handle one input element at a time, and will produce\n",
      " |      identical results = to @{tf.data.Dataset.flat_map}. In general,\n",
      " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
      " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
      " |      producing `block_length` consecutive elements from each iterator, and\n",
      " |      consuming the next input element each time it reaches the end of an\n",
      " |      iterator.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3, 4, 5 }\n",
      " |      \n",
      " |      # NOTE: New lines indicate \"block\" boundaries.\n",
      " |      a.interleave(lambda x: Dataset.from_tensors(x).repeat(6),\n",
      " |                   cycle_length=2, block_length=4) == {\n",
      " |          1, 1, 1, 1,\n",
      " |          2, 2, 2, 2,\n",
      " |          1, 1,\n",
      " |          2, 2,\n",
      " |          3, 3, 3, 3,\n",
      " |          4, 4, 4, 4,\n",
      " |          3, 3,\n",
      " |          4, 4,\n",
      " |          5, 5, 5, 5,\n",
      " |          5, 5,\n",
      " |      }\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The order of elements yielded by this transformation is\n",
      " |      deterministic, as long as `map_func` is a pure function. If\n",
      " |      `map_func` contains any stateful operations, the order in which\n",
      " |      that state is accessed is undefined.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having shapes\n",
      " |          and types defined by `self.output_shapes` and `self.output_types`) to a\n",
      " |          `Dataset`.\n",
      " |        cycle_length: The number of elements from this dataset that will be\n",
      " |          processed concurrently.\n",
      " |        block_length: The number of consecutive elements to produce from each\n",
      " |          input element before cycling to another input element.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  make_initializable_iterator(self, shared_name=None)\n",
      " |      Creates an `Iterator` for enumerating the elements of this dataset.\n",
      " |      \n",
      " |      Note: The returned iterator will be in an uninitialized state,\n",
      " |      and you must run the `iterator.initializer` operation before using it:\n",
      " |      \n",
      " |      ```python\n",
      " |      dataset = ...\n",
      " |      iterator = dataset.make_initializable_iterator()\n",
      " |      # ...\n",
      " |      sess.run(iterator.initializer)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        shared_name: (Optional.) If non-empty, the returned iterator will be\n",
      " |          shared under the given name across multiple sessions that share the\n",
      " |          same devices (e.g. when using a remote server).\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Iterator` over the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If eager execution is enabled.\n",
      " |  \n",
      " |  make_one_shot_iterator(self)\n",
      " |      Creates an `Iterator` for enumerating the elements of this dataset.\n",
      " |      \n",
      " |      Note: The returned iterator will be initialized automatically.\n",
      " |      A \"one-shot\" iterator does not currently support re-initialization.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Iterator` over the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If eager execution is enabled.\n",
      " |  \n",
      " |  map(self, map_func, num_parallel_calls=None)\n",
      " |      Maps `map_func` across this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a nested structure of tensors (having\n",
      " |          shapes and types defined by `self.output_shapes` and\n",
      " |         `self.output_types`) to another nested structure of tensors.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n",
      " |          representing the number elements to process in parallel. If not\n",
      " |          specified, elements will be processed sequentially.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  padded_batch(self, batch_size, padded_shapes, padding_values=None)\n",
      " |      Combines consecutive elements of this dataset into padded batches.\n",
      " |      \n",
      " |      This transformation combines multiple consecutive elements of the input\n",
      " |      dataset into a single element. Like @{tf.data.Dataset.batch}, the tensors\n",
      " |      in the resulting element have an additional outer dimension, which will be\n",
      " |      `batch_size` for all but the last element, and `N % batch_size` for the\n",
      " |      last element (where `N` is the number of elements in this dataset). Unlike\n",
      " |      @{tf.data.Dataset.batch}, the elements may have different shapes for some\n",
      " |      of their components, and this transformation will pad each component to\n",
      " |      the respective shape in `padding_shapes`. The `padding_shapes` argument\n",
      " |      determines the resulting shape for each dimension of each component in an\n",
      " |      output element:\n",
      " |      \n",
      " |      * If the dimension is a constant (e.g. `tf.Dimension(37)`), the component\n",
      " |        will be padded out to that length in that dimension.\n",
      " |      * If the dimension is unknown (e.g. `tf.Dimension(None)`), the component\n",
      " |        will be padded out to the maximum length of all elements in that\n",
      " |        dimension.\n",
      " |      \n",
      " |      NOTE: If the number of elements (`N`) in this dataset is not an exact\n",
      " |      multiple of `batch_size`, the final batch contain smaller tensors with\n",
      " |      shape `N % batch_size` in the batch dimension. If your program depends on\n",
      " |      the batches having the same shape, consider using the\n",
      " |      @{tf.contrib.data.padded_batch_and_drop_remainder} transformation instead.\n",
      " |      \n",
      " |      See also @{tf.contrib.data.dense_to_sparse_batch}, which combines elements\n",
      " |      that may have different shapes into a @{tf.SparseTensor}.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        padded_shapes: A nested structure of `tf.TensorShape` or\n",
      " |          `tf.int64` vector tensor-like objects representing the shape\n",
      " |          to which the respective component of each input element should\n",
      " |          be padded prior to batching. Any unknown dimensions\n",
      " |          (e.g. `tf.Dimension(None)` in a `tf.TensorShape` or `-1` in a\n",
      " |          tensor-like object) will be padded to the maximum size of that\n",
      " |          dimension in each batch.\n",
      " |        padding_values: (Optional.) A nested structure of scalar-shaped\n",
      " |          `tf.Tensor`, representing the padding values to use for the\n",
      " |          respective components.  Defaults are `0` for numeric types and\n",
      " |          the empty string for string types.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  prefetch(self, buffer_size)\n",
      " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          maximum number elements that will be buffered when prefetching.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  repeat(self, count=None)\n",
      " |      Repeats this dataset `count` times.\n",
      " |      \n",
      " |      NOTE: If this dataset is a function of global state (e.g. a random number\n",
      " |      generator), then different repetitions may produce different elements.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of times the dataset should be repeated. The default behavior\n",
      " |          (if `count` is `None` or `-1`) is for the dataset be repeated\n",
      " |          indefinitely.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  shard(self, num_shards, index)\n",
      " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      " |      \n",
      " |      This dataset operator is very useful when running distributed training, as\n",
      " |      it allows each worker to read a unique subset.\n",
      " |      \n",
      " |      When reading a single input file, you can skip elements as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.TFRecordDataset(FLAGS.input_file)\n",
      " |      d = d.shard(FLAGS.num_workers, FLAGS.worker_index)\n",
      " |      d = d.repeat(FLAGS.num_epochs)\n",
      " |      d = d.shuffle(FLAGS.shuffle_buffer_size)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Important caveats:\n",
      " |      \n",
      " |      - Be sure to shard before you use any randomizing operator (such as\n",
      " |        shuffle).\n",
      " |      - Generally it is best if the shard operator is used early in the dataset\n",
      " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
      " |        before converting the dataset to input samples. This avoids reading every\n",
      " |        file on every worker. The following is an example of an efficient\n",
      " |        sharding strategy within a complete pipeline:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = Dataset.list_files(FLAGS.pattern)\n",
      " |      d = d.shard(FLAGS.num_workers, FLAGS.worker_index)\n",
      " |      d = d.repeat(FLAGS.num_epochs)\n",
      " |      d = d.shuffle(FLAGS.shuffle_buffer_size)\n",
      " |      d = d.repeat()\n",
      " |      d = d.interleave(tf.data.TFRecordDataset,\n",
      " |                       cycle_length=FLAGS.num_readers, block_length=1)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          shards operating in parallel.\n",
      " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `num_shards` or `index` are illegal values. Note: error\n",
      " |          checking is done on a best-effort basis, and aren't guaranteed to be\n",
      " |          caught upon dataset creation. (e.g. providing in a placeholder tensor\n",
      " |          bypasses the early checking, and will instead result in an error during\n",
      " |          a session.run call.)\n",
      " |  \n",
      " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
      " |      Randomly shuffles the elements of this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of elements from this dataset from which the new\n",
      " |          dataset will sample.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          random seed that will be used to create the distribution. See\n",
      " |          @{tf.set_random_seed} for behavior.\n",
      " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
      " |          iterated over. (Defaults to `True`.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  skip(self, count)\n",
      " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number\n",
      " |          of elements of this dataset that should be skipped to form the\n",
      " |          new dataset.  If `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain no elements.  If `count`\n",
      " |          is -1, skips the entire dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  take(self, count)\n",
      " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be taken to form the new dataset.\n",
      " |          If `count` is -1, or if `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain all elements of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_generator(generator, output_types, output_shapes=None)\n",
      " |      Creates a `Dataset` whose elements are generated by `generator`.\n",
      " |      \n",
      " |      The `generator` argument must be a callable object that returns\n",
      " |      an object that support the `iter()` protocol (e.g. a generator function).\n",
      " |      The elements generated by `generator` must be compatible with the given\n",
      " |      `output_types` and (optional) `output_shapes` arguments.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      import itertools\n",
      " |      \n",
      " |      def gen():\n",
      " |        for i in itertools.count(1):\n",
      " |          yield (i, [1] * i)\n",
      " |      \n",
      " |      ds = Dataset.from_generator(\n",
      " |          gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\n",
      " |      value = ds.make_one_shot_iterator().get_next()\n",
      " |      \n",
      " |      sess.run(value)  # (1, array([1]))\n",
      " |      sess.run(value)  # (2, array([1, 1]))\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The current implementation of `Dataset.from_generator()` uses\n",
      " |      @{tf.py_func} and inherits the same constraints. In particular, it\n",
      " |      requires the `Dataset`- and `Iterator`-related operations to be placed\n",
      " |      on a device in the same process as the Python program that called\n",
      " |      `Dataset.from_generator()`. The body of `generator` will not be\n",
      " |      serialized in a `GraphDef`, and you should not use this method if you\n",
      " |      need to serialize your model and restore it in a different environment.\n",
      " |      \n",
      " |      NOTE: If `generator` depends on mutable global variables or other external\n",
      " |      state, be aware that the runtime may invoke `generator` multiple times\n",
      " |      (in order to support repeating the `Dataset`) and at any time\n",
      " |      between the call to `Dataset.from_generator()` and the production of the\n",
      " |      first element from the generator. Mutating global variables or external\n",
      " |      state can cause undefined behavior, and we recommend that you explicitly\n",
      " |      cache any external state in `generator` before calling\n",
      " |      `Dataset.from_generator()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        generator: A callable object that takes no arguments and returns an\n",
      " |          object that supports the `iter()` protocol.\n",
      " |        output_types: A nested structure of `tf.DType` objects corresponding to\n",
      " |          each component of an element yielded by `generator`.\n",
      " |        output_shapes: (Optional.) A nested structure of `tf.TensorShape`\n",
      " |          objects corresponding to each component of an element yielded by\n",
      " |          `generator`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_sparse_tensor_slices(sparse_tensor)\n",
      " |      Splits each rank-N `tf.SparseTensor` in this dataset row-wise. (deprecated)\n",
      " |      \n",
      " |      THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use `tf.data.Dataset.from_tensor_slices()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_tensor: A `tf.SparseTensor`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` of rank-(N-1) sparse tensors.\n",
      " |  \n",
      " |  from_tensor_slices(tensors)\n",
      " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A nested structure of tensors, each having the same size in the\n",
      " |          0th dimension.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensors(tensors)\n",
      " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A nested structure of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  list_files(file_pattern, shuffle=None)\n",
      " |      A dataset of all files matching a pattern.\n",
      " |      \n",
      " |      Example:\n",
      " |        If we had the following files on our filesystem:\n",
      " |          - /path/to/dir/a.txt\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset would\n",
      " |        produce:\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |      NOTE: The order of the file names returned can be non-deterministic even\n",
      " |      when `shuffle` is `False`.\n",
      " |      \n",
      " |      Args:\n",
      " |        file_pattern: A string or scalar string `tf.Tensor`, representing\n",
      " |          the filename pattern that will be matched.\n",
      " |        shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
      " |          Defaults to `True`.\n",
      " |      \n",
      " |      Returns:\n",
      " |       Dataset: A `Dataset` of strings corresponding to file names.\n",
      " |  \n",
      " |  range(*args)\n",
      " |      Creates a `Dataset` of a step-separated range of values.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      Dataset.range(5) == [0, 1, 2, 3, 4]\n",
      " |      Dataset.range(2, 5) == [2, 3, 4]\n",
      " |      Dataset.range(1, 5, 2) == [1, 3]\n",
      " |      Dataset.range(1, 5, -2) == []\n",
      " |      Dataset.range(5, 1) == []\n",
      " |      Dataset.range(5, 1, -2) == [5, 3]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: follow same semantics as python's xrange.\n",
      " |          len(args) == 1 -> start = 0, stop = args[0], step = 1\n",
      " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1\n",
      " |          len(args) == 3 -> start = args[0], stop = args[1, stop = args[2]\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `RangeDataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if len(args) == 0.\n",
      " |  \n",
      " |  zip(datasets)\n",
      " |      Creates a `Dataset` by zipping together the given datasets.\n",
      " |      \n",
      " |      This method has similar semantics to the built-in `zip()` function\n",
      " |      in Python, with the main difference being that the `datasets`\n",
      " |      argument can be an arbitrary nested structure of `Dataset` objects.\n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3 }\n",
      " |      b = { 4, 5, 6 }\n",
      " |      c = { (7, 8), (9, 10), (11, 12) }\n",
      " |      d = { 13, 14 }\n",
      " |      \n",
      " |      # The nested structure of the `datasets` argument determines the\n",
      " |      # structure of elements in the resulting dataset.\n",
      " |      Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }\n",
      " |      Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }\n",
      " |      \n",
      " |      # The `datasets` argument may contain an arbitrary number of\n",
      " |      # datasets.\n",
      " |      Dataset.zip((a, b, c)) == { (1, 4, (7, 8)),\n",
      " |                                  (2, 5, (9, 10)),\n",
      " |                                  (3, 6, (11, 12)) }\n",
      " |      \n",
      " |      # The number of elements in the resulting dataset is the same as\n",
      " |      # the size of the smallest dataset in `datasets`.\n",
      " |      Dataset.zip((a, d)) == { (1, 13), (2, 14) }\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        datasets: A nested structure of datasets.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  output_classes\n",
      " |      Returns the class of each component of an element of this dataset.\n",
      " |      \n",
      " |      The expected values are `tf.Tensor` and `tf.SparseTensor`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of Python `type` objects corresponding to each\n",
      " |        component of an element of this dataset.\n",
      " |  \n",
      " |  output_shapes\n",
      " |      Returns the shape of each component of an element of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.TensorShape` objects corresponding to each\n",
      " |        component of an element of this dataset.\n",
      " |  \n",
      " |  output_types\n",
      " |      Returns the type of each component of an element of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.DType` objects corresponding to each component\n",
      " |        of an element of this dataset.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.float32, tf.float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = tf.gfile.Glob(os.path.join(\"\", \"mnist\", \"sample*\"))\n",
    "dataset_ = tf.data.TFRecordDataset(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.readers.TFRecordDataset"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_features = {\"0_dense_input\": tf.FixedLenSequenceFeature((28,28), dtype=tf.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected string passed to parameter 'serialized' of op 'ParseSingleSequenceExample', got <TFRecordDataset shapes: (), types: tf.string> of type 'TFRecordDataset' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    343\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 344\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string, got <TFRecordDataset shapes: (), types: tf.string> of type 'TFRecordDataset' instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-2e90f81bfe9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m\"label_score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVarLenFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         },\n\u001b[0;32m----> 7\u001b[0;31m         sequence_features=sequence_features)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_sequence_example\u001b[0;34m(serialized, context_features, sequence_features, example_name, name)\u001b[0m\n\u001b[1;32m    950\u001b[0m       \u001b[0mfeature_list_sparse_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list_dense_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       \u001b[0mfeature_list_dense_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list_dense_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       feature_list_dense_defaults, example_name, name)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36m_parse_single_sequence_example_raw\u001b[0;34m(serialized, context_sparse_keys, context_sparse_types, context_dense_keys, context_dense_types, context_dense_defaults, context_dense_shapes, feature_list_sparse_keys, feature_list_sparse_types, feature_list_dense_keys, feature_list_dense_types, feature_list_dense_shapes, feature_list_dense_defaults, debug_name, name)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         feature_list_dense_missing_assumed_empty=(\n\u001b[1;32m   1147\u001b[0m             feature_list_dense_missing_assumed_empty),\n\u001b[0;32m-> 1148\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m     (context_sparse_indices, context_sparse_values,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_sequence_example\u001b[0;34m(serialized, feature_list_dense_missing_assumed_empty, context_sparse_keys, context_dense_keys, feature_list_sparse_keys, feature_list_dense_keys, context_dense_defaults, debug_name, context_sparse_types, feature_list_dense_types, context_dense_shapes, feature_list_sparse_types, feature_list_dense_shapes, name)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mcontext_dense_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_dense_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mfeature_list_sparse_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_list_sparse_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         feature_list_dense_shapes=feature_list_dense_shapes, name=name)\n\u001b[0m\u001b[1;32m    844\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    517\u001b[0m                   \u001b[0;34m\"type '%s' instead.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                   (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,\n\u001b[0;32m--> 519\u001b[0;31m                    repr(values), type(values).__name__))\n\u001b[0m\u001b[1;32m    520\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string passed to parameter 'serialized' of op 'ParseSingleSequenceExample', got <TFRecordDataset shapes: (), types: tf.string> of type 'TFRecordDataset' instead."
     ]
    }
   ],
   "source": [
    "contexts, features = tf.parse_single_sequence_example(\n",
    "        dataset_,\n",
    "        context_features={\n",
    "            \"label_index\": tf.VarLenFeature(tf.int64),\n",
    "            \"label_score\": tf.VarLenFeature(tf.float32)\n",
    "        },\n",
    "        sequence_features=sequence_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = dataset_.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(30, 1, 28, 28)\n",
      "(30, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    haha = sess.run(iterator.get_next())\n",
    "    print(type(haha[0]))\n",
    "    print(haha[0].shape)\n",
    "    print(haha[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(30, 784)\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    haha = sess.run(example_flatten)\n",
    "    print(type(haha[0]))\n",
    "    print(haha.shape)\n",
    "    print(haha[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-7302ac88ae36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "  while True:\n",
    "    try:\n",
    "      sess.run(next_example)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert MNIST to TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load /Users/evariste/projects/code-exercises/tensorflow/convert_to_records.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.SequenceExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_sets = mnist.read_data_sets(FLAGS.directory,\n",
    "                                   dtype=tf.uint8,\n",
    "                                   reshape=False,\n",
    "                                   validation_size=FLAGS.validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "next_batch() missing 1 required positional argument: 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-bc67e5746179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: next_batch() missing 1 required positional argument: 'batch_size'"
     ]
    }
   ],
   "source": [
    "data_sets.train.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/tmp/data/train.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haha = tf.data.TFRecordDataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = haha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-947c347af23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_single_sequence_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhaha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_sequence_example\u001b[0;34m(serialized, context_features, sequence_features, example_name, name)\u001b[0m\n\u001b[1;32m    934\u001b[0m   \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_features\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msequence_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing features.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m   (context_sparse_keys, context_sparse_types, context_dense_keys,\n\u001b[1;32m    938\u001b[0m    \u001b[0mcontext_dense_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_dense_defaults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Missing features."
     ]
    }
   ],
   "source": [
    "contexts, features = tf.parse_single_sequence_example(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-6a7f1b0d1fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m features=tf.train.Features(\n\u001b[1;32m      2\u001b[0m               feature={\n\u001b[0;32m----> 3\u001b[0;31m                   \u001b[0;34m'height'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_int64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                   \u001b[0;34m'width'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_int64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0;34m'depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_int64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rows' is not defined"
     ]
    }
   ],
   "source": [
    "features=tf.train.Features(\n",
    "              feature={\n",
    "                  'height': _int64_feature(rows),\n",
    "                  'width': _int64_feature(cols),\n",
    "                  'depth': _int64_feature(depth),\n",
    "                  'label': _int64_feature(int(labels[index])),\n",
    "                  'image_raw': _bytes_feature(image_raw)\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_addrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-73d04b3f5bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_addrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# print how many images are saved every 1000 images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_addrs' is not defined"
     ]
    }
   ],
   "source": [
    "train_filename = 'train.tfrecords'  # address to save the TFRecords file\n",
    "\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "\n",
    "for i in range(len(train_addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if not i % 1000:\n",
    "        print('Train data: {}/{}'.format(i, len(train_addrs)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Load the image\n",
    "    img = load_image(train_addrs[i])\n",
    "\n",
    "    label = train_labels[i]\n",
    "\n",
    "    # Create a feature\n",
    "    feature = {'train/label': _int64_feature(label),\n",
    "               'train/image': _bytes_feature(tf.compat.as_bytes(img.tostring()))}\n",
    "\n",
    "    # Create an example protocol buffer\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    # Serialize to string and write on the file\n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c306bc160>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX4gKMIPBPV/h887MJ6UHykAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c3b77c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = np.random.choice(len(data_sets.train.images))\n",
    "image = data_sets.train.images[index].reshape((28,28))\n",
    "\n",
    "fig = plt.gca()\n",
    "fig.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets.train.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Name of dataset is not found: cifar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-4d2fab5c919e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcifar10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cifar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m               instructions)\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(name, size, test_with_fake_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m   \"\"\"\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Name of dataset is not found: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dbpedia'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_with_fake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Name of dataset is not found: cifar"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.contrib.learn.datasets.load_dataset(\"cifar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def read_and_decode(filename_queue):\n",
    "  reader = tf.TFRecordReader()\n",
    "  _, serialized_example = reader.read(filename_queue)\n",
    "  features = tf.parse_single_sequence_example(\n",
    "          serialized_example,\n",
    "          context_features={\n",
    "              'label_index': tf.FixedLenFeature(shape=[1], dtype=tf.int64),\n",
    "              'label_score': tf.FixedLenFeature(shape=[1], dtype=tf.float32)},\n",
    "          sequence_features={\n",
    "              '0_dense_input': tf.FixedLenSequenceFeature(\n",
    "                  shape=[28*28], dtype=tf.float32)}) #image in MNIST is of size 28 by 28\n",
    "  # print ('features: ', features)\n",
    "\n",
    "  return features[1]['0_dense_input'], features[0]['label_index']\n",
    "\n",
    "\n",
    "def read_inputs(inputfile_pattern, batch_size=1000): \n",
    "  \"\"\"\n",
    "  Arguments: \n",
    "  - inputfile_pattern: path to the input tfrecord file\n",
    "  - batch_size: specify how many images the reader will read\n",
    "\n",
    "  Returns:\n",
    "  - dense_inputs_flatten: normalized flatten image matrix, of shape (784, batch_size)\n",
    "  - one_hot_labels: one-hotted labels\n",
    "  - labels: raw labels\n",
    "\n",
    "  Raise:\n",
    "  IOError\n",
    "  \"\"\"\n",
    "  if not os.path.isfile(inputfile_pattern):\n",
    "    raise IOError(\"Unable to find training files. inputfile_pattern='\" +\n",
    "                    inputfile_pattern + \"'.\")\n",
    "  filename_queue = tf.train.string_input_producer([inputfile_pattern])\n",
    "\n",
    "  dense_input, label = read_and_decode(filename_queue)\n",
    "  \n",
    "  dense_input = tf.reshape(dense_input, [1, 28*28])\n",
    "  label = tf.reshape(label, [1, ])\n",
    "  \n",
    "  _batch_size = batch_size\n",
    "  dense_inputs, labels = tf.train.shuffle_batch(\n",
    "          [dense_input, label], batch_size=_batch_size,\n",
    "          capacity=1 + 3*_batch_size, min_after_dequeue=1)\n",
    "  dense_inputs_flatten = tf.transpose(tf.squeeze(dense_inputs))\n",
    "\n",
    "  dense_inputs_flatten /= 255.\n",
    "  one_hot_labels = tf.transpose(tf.squeeze(tf.one_hot(labels, 10))) \n",
    "  return dense_inputs_flatten, one_hot_labels, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/data/train.tfrecords'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'queue_ref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-d380873af9f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-256-07f293a34372>\u001b[0m in \u001b[0;36mread_and_decode\u001b[0;34m(filename_queue)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_and_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   features = tf.parse_single_sequence_example(\n\u001b[1;32m     14\u001b[0m           \u001b[0mserialized_example\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/io_ops.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, queue, name)\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mqueue_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m       \u001b[0mqueue_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_read_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'queue_ref'"
     ]
    }
   ],
   "source": [
    "dense_input, label = read_and_decode(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"file0.csv\", \"file1.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.data_flow_ops.FIFOQueue at 0x1c30a63550>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReaderReadV2:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.SequenceExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.parse_single_sequence_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haha = tf.VarLenFeature(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarLenFeature(dtype=tf.float32)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_ = AutoDLMetadata(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _feature_key(index, feature_name):\n",
    "    return str(index) + \"_\" + feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_file_pattern(dataset_name):\n",
    "  return os.path.join(\"\", dataset_name, \"sample*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = tf.gfile.Glob(dataset_file_pattern(\"mnist\"))\n",
    "sequence_example_proto = tf.data.TFRecordDataset(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected string passed to parameter 'serialized' of op 'ParseSingleSequenceExample', got <TFRecordDataset shapes: (), types: tf.string> of type 'TFRecordDataset' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    343\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 344\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string, got <TFRecordDataset shapes: (), types: tf.string> of type 'TFRecordDataset' instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-279-ddd886f9fe05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m\"label_score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVarLenFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     },\n\u001b[0;32m---> 23\u001b[0;31m     sequence_features=sequence_features)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_sequence_example\u001b[0;34m(serialized, context_features, sequence_features, example_name, name)\u001b[0m\n\u001b[1;32m    950\u001b[0m       \u001b[0mfeature_list_sparse_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list_dense_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       \u001b[0mfeature_list_dense_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list_dense_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       feature_list_dense_defaults, example_name, name)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36m_parse_single_sequence_example_raw\u001b[0;34m(serialized, context_sparse_keys, context_sparse_types, context_dense_keys, context_dense_types, context_dense_defaults, context_dense_shapes, feature_list_sparse_keys, feature_list_sparse_types, feature_list_dense_keys, feature_list_dense_types, feature_list_dense_shapes, feature_list_dense_defaults, debug_name, name)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         feature_list_dense_missing_assumed_empty=(\n\u001b[1;32m   1147\u001b[0m             feature_list_dense_missing_assumed_empty),\n\u001b[0;32m-> 1148\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m     (context_sparse_indices, context_sparse_values,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_sequence_example\u001b[0;34m(serialized, feature_list_dense_missing_assumed_empty, context_sparse_keys, context_dense_keys, feature_list_sparse_keys, feature_list_dense_keys, context_dense_defaults, debug_name, context_sparse_types, feature_list_dense_types, context_dense_shapes, feature_list_sparse_types, feature_list_dense_shapes, name)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mcontext_dense_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_dense_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mfeature_list_sparse_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_list_sparse_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         feature_list_dense_shapes=feature_list_dense_shapes, name=name)\n\u001b[0m\u001b[1;32m    844\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    517\u001b[0m                   \u001b[0;34m\"type '%s' instead.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                   (dtypes.as_dtype(dtype).name, input_arg.name, op_type_name,\n\u001b[0;32m--> 519\u001b[0;31m                    repr(values), type(values).__name__))\n\u001b[0m\u001b[1;32m    520\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string passed to parameter 'serialized' of op 'ParseSingleSequenceExample', got <TFRecordDataset shapes: (), types: tf.string> of type 'TFRecordDataset' instead."
     ]
    }
   ],
   "source": [
    "sequence_features = {}\n",
    "for i in range(metadata_.get_bundle_size()):\n",
    "  if metadata_.is_sparse(i):\n",
    "    sequence_features[_feature_key(\n",
    "        i, \"sparse_col_index\")] = tf.VarLenFeature(tf.int64)\n",
    "    sequence_features[_feature_key(\n",
    "        i, \"sparse_row_index\")] = tf.VarLenFeature(tf.int64)\n",
    "    sequence_features[_feature_key(\n",
    "        i, \"sparse_value\")] = tf.VarLenFeature(tf.float32)\n",
    "  if metadata_.is_compressed(i):\n",
    "    sequence_features[_feature_key(\n",
    "        i, \"compressed\")] = tf.VarLenFeature(tf.string)\n",
    "  else:\n",
    "    sequence_features[_feature_key(\n",
    "        i, \"dense_input\")] = tf.FixedLenSequenceFeature(\n",
    "            metadata_.get_matrix_size(i), dtype=tf.float32)\n",
    "contexts, features = tf.parse_single_sequence_example(\n",
    "    sequence_example_proto,\n",
    "    context_features={\n",
    "        \"label_index\": tf.VarLenFeature(tf.int64),\n",
    "        \"label_score\": tf.VarLenFeature(tf.float32)\n",
    "    },\n",
    "    sequence_features=sequence_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_data_sets() missing 1 required positional argument: 'train_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-01c884d26586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m               instructions)\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_data_sets() missing 1 required positional argument: 'train_dir'"
     ]
    }
   ],
   "source": [
    "dataset = mnist.read_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'mnist/sample-00000-of-00007'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haha = tf.data.TFRecordDataset(filenames=[filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = AutoDLDataset('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## ['mnist/sample-00002-of-00007', 'mnist/sample-00003-of-00007', 'mnist/sample-00005-of-00007', 'mnist/sample-00004-of-00007', 'mnist/sample-00001-of-00007', 'mnist/sample-00000-of-00007', 'mnist/sample-00006-of-00007']\n",
      "INFO:tensorflow:Number of training files: 7.\n",
      "************************************************** <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>\n"
     ]
    }
   ],
   "source": [
    "d.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>\n"
     ]
    }
   ],
   "source": [
    "hahaha = haha.map(d._parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = hahaha.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of file!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "num = -1\n",
    "while True:\n",
    "    try:\n",
    "        example = sess.run(iterator.get_next())\n",
    "        os.sys('cls')\n",
    "        num += 1\n",
    "#         print('Example', example)\n",
    "        print(num)\n",
    "    except:\n",
    "        print(\"End of file!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of file!\n"
     ]
    }
   ],
   "source": [
    "dataset_iterator = hahaha.make_one_shot_iterator()\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  iterator = dataset_iterator.get_next()\n",
    "  # We add a limited number of examples.\n",
    "  for _ in range(1000000):\n",
    "    try:\n",
    "      input_minibatch, output_minibatch = sess.run(iterator)\n",
    "    except:\n",
    "      print(\"End of file!\")\n",
    "      break\n",
    "    for inp in input_minibatch:\n",
    "      # We flatten examples.\n",
    "      inputs.append(inp)\n",
    "      outputs.append(output_minibatch)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and convert MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evariste/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/evariste/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "datasets = mnist.read_data_sets(train_dir='/tmp/data/', one_hot=False, validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(0, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(datasets.train.images.shape)\n",
    "print(datasets.validation.images.shape)\n",
    "print(datasets.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(0,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(datasets.train.labels.shape)\n",
    "print(datasets.validation.labels.shape)\n",
    "print(datasets.test.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating MNIST-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = datasets.train.images\n",
    "output_sequence = datasets.train.labels\n",
    "num_examples = len(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(input_sequence.shape)\n",
    "print(output_sequence.shape)\n",
    "print(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  # here value is a list\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _feature_list(feature):\n",
    "  return tf.train.FeatureList(feature=[feature])\n",
    "\n",
    "def convert_to_sequence_example_tfrecords(features, labels, filename):\n",
    "  num_examples = features.shape[0]\n",
    "  if num_examples != labels.shape[0]:\n",
    "    raise ValueError('Features size %d does not match labels size %d.' %\n",
    "                     (num_examples, labels.shape[0]))\n",
    "  print('Writing', filename)\n",
    "  with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "    for index in range(num_examples):\n",
    "      context = tf.train.Features(\n",
    "        feature={\n",
    "          'label_index': _int64_feature(labels[index]),\n",
    "          'label_score': _float_feature([1])\n",
    "        })\n",
    "      feature_lists = tf.train.FeatureLists(\n",
    "          feature_list={\n",
    "          '0_dense_input': _feature_list(_float_feature(features[index]))\n",
    "          })\n",
    "      sequence_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists)\n",
    "      writer.write(sequence_example.SerializeToString())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "convert_to_sequence_example_tfrecords(input_sequence, output_sequence, 'sample-00000-of-00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_sequence_example_tfrecords_test(features, labels, filename):\n",
    "  num_examples = features.shape[0]\n",
    "  if num_examples != labels.shape[0]:\n",
    "    raise ValueError('Features size %d does not match labels size %d.' %\n",
    "                     (num_examples, labels.shape[0]))\n",
    "  print('Writing', filename)\n",
    "  with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "    for index in range(num_examples):\n",
    "      context = tf.train.Features(\n",
    "        feature={\n",
    "          'label_index': _int64_feature(0),\n",
    "          'label_score': _float_feature([0])\n",
    "        })\n",
    "      feature_lists = tf.train.FeatureLists(\n",
    "          feature_list={\n",
    "          '0_dense_input': _feature_list(_float_feature(features[index]))\n",
    "          })\n",
    "      sequence_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists)\n",
    "      writer.write(sequence_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample-00000-of-00001-test\n"
     ]
    }
   ],
   "source": [
    "input_sequence = datasets.test.images\n",
    "output_sequence = datasets.test.labels\n",
    "\n",
    "convert_to_sequence_example_tfrecords_test(input_sequence, output_sequence, 'sample-00000-of-00001-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"mnist.solution\", output_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
